{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f261de1a",
   "metadata": {},
   "source": [
    "# Semantic Kernel OpenTelemetry full-instrumentation example\n",
    "\n",
    "## Context\n",
    "\n",
    "Given the incresing complexity of GenAI applications, it is important to have a way to monitor and trace the interactions between different components of the system. OpenTelemetry is a set of APIs, libraries, agents, and instrumentation that provide observability for applications. In this example, we will demonstrate how to use OpenTelemetry with Semantic Kernel to trace the execution of a simple AI Agent, **running the full stack locally using Docker and Aspire dashboard.**\n",
    "\n",
    "Please read the main documentation about [OpenTelemetry and Semantic Kernel](https://review.learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/telemetry-with-app-insights?branch=main&tabs=Powershell&pivots=programming-language-python) for additional information.\n",
    "\n",
    "## Prerequisites\n",
    "- Docker installed\n",
    "- Python 3.12 or later\n",
    "\n",
    "## Install the required packages\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Run the Aspire dashboard (Docker container)\n",
    "```bash\n",
    "docker run --rm -it \\\n",
    "-p 18888:18888 \\\n",
    "-p 4317:18889 \\\n",
    "--name aspire-dashboard \\\n",
    "mcr.microsoft.com/dotnet/aspire-dashboard:9.0\n",
    "```\n",
    "\n",
    "## Check environment variables\n",
    "\n",
    "Ensure you have a `.env` file in this folder, copied from `.env.example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE logging must be configured before importing any other modules\n",
    "# to ensure that all loggers are configured correctly\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080235e",
   "metadata": {},
   "source": [
    "## Key technical note\n",
    "\n",
    "By default, Semantic Kernel outputs two different types of telemetry data: `traces` and `logs`. Agent invocations are _traced_, while chat completions are _logged_ - since their size may exceed limitations/support of collectors. \n",
    "\n",
    "Most OpenTelemetry backends (like Jaeger, Zipkin or MLFlow) do not capture logs, while Aspire dashboard does but does not visualize them in the same way as traces, which can be confusing for developers trying to understand and debug the flow of their application locally.\n",
    "\n",
    "Fortunately, we can configure Semantic Kernel to output traces instead of logs. This is done by the custom `LogToSpanExporter` class, which is a custom OpenTelemetry exporter that converts logs to spans. This allows us to send all telemetry data to Jaeger for visualization and analysis.\n",
    "\n",
    "**Note**: this solution is not recommended for production use, as it may lead to performance issues and increased complexity. It is only intended for local development and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk._logs import LogData\n",
    "from opentelemetry.sdk._logs.export import LogExporter, LogExportResult\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LogToSpanExporter(LogExporter):\n",
    "    def __init__(self, tracer=None):\n",
    "        self._tracer = tracer or trace.get_tracer(__name__)\n",
    "        logger.info(f\"Using tracer: {self._tracer}\")\n",
    "\n",
    "    def export(self, batch: list[LogData]) -> \"LogExportResult\":\n",
    "        \"\"\"\n",
    "        Called by the LogPipeline when a batch of logs is ready.\n",
    "        We convert each LogRecord into its own span.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Exporting {len(batch)} log records as spans\")\n",
    "        for data in batch:\n",
    "            try:\n",
    "                if not isinstance(data, LogData):\n",
    "                    logger.warning(f\"Skipping non-log record: {data}\")\n",
    "                    continue\n",
    "                # Create a span for each log record\n",
    "                span_name = data.log_record.attributes.get(\"code.function\", \"log\")\n",
    "                with self._tracer.start_as_current_span(\n",
    "                    span_name,\n",
    "                    kind=trace.SpanKind.INTERNAL,\n",
    "                    attributes={\n",
    "                        \"log.severity\": data.log_record.severity_text,\n",
    "                        \"log.message\": data.log_record.body,\n",
    "                        **{f\"log.attr.{k}\": v for k, v in (data.log_record.attributes or {}).items()},\n",
    "                    },\n",
    "                    start_time=data.log_record.timestamp,    # uses the logâ€™s timestamp\n",
    "                ) as span:\n",
    "                    logger.info(f\"Exporting log record as span: {span_name}\")\n",
    "                    # Optionally, mark error spans\n",
    "                    if data.log_record.severity_number.value >= 400:\n",
    "                        span.set_status(trace.Status(trace.StatusCode.ERROR))\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Failed to export log record as span: {e}\")\n",
    "                return LogExportResult.FAILURE\n",
    "                        \n",
    "            return LogExportResult.SUCCESS\n",
    "\n",
    "    def shutdown(self):\n",
    "        # Clean up if needed\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb3f8c",
   "metadata": {},
   "source": [
    "## Setting up tracing (spans)\n",
    "\n",
    "First step is to set up the OpenTelemetry SDK to export traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "\n",
    "# 1. Set up the provider and exporter (OTLP exporter uses env vars for config)\n",
    "tracer_provider = TracerProvider()\n",
    "otlp_exporter = OTLPSpanExporter()\n",
    "processor = SimpleSpanProcessor(otlp_exporter)\n",
    "\n",
    "tracer_provider.add_span_processor(processor)\n",
    "\n",
    "# 2. Register the provider as global\n",
    "trace.set_tracer_provider(tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb4fca",
   "metadata": {},
   "source": [
    "## Setup logging\n",
    "\n",
    "This is where we inject the custom `LogToSpanExporter` class to convert logs to spans. This allows us to send all telemetry data to Jaeger for visualization and analysis.\n",
    "\n",
    "**NOTE** you can run a comparision with the default `OTLPLogExporter` class, which will output the logs in the Aspire dashboard but not in the timeline view. \n",
    "\n",
    "**NOTE #2** when you change the `OTLPLogExporter` to `LogToSpanExporter`, you will need to restart the Jupyter kernel to see the changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5433d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from opentelemetry._logs import set_logger_provider\n",
    "from opentelemetry.sdk._logs import LoggerProvider, LoggingHandler\n",
    "from opentelemetry.sdk._logs.export import SimpleLogRecordProcessor\n",
    "from opentelemetry.exporter.otlp.proto.grpc._log_exporter import OTLPLogExporter\n",
    "from opentelemetry.sdk._logs.export import ConsoleLogExporter\n",
    "\n",
    "logger_provider = LoggerProvider()\n",
    "\n",
    "# ðŸ“¢ Custom log exporter that sends logs to the OTLP exporter as spans\n",
    "log_exporter = LogToSpanExporter()\n",
    "\n",
    "# ðŸ“¢ Or use the OTLPLogExporter directly if you want to send logs in OTLP format\n",
    "# log_exporter = OTLPLogExporter()\n",
    "\n",
    "logger_provider.add_log_record_processor(SimpleLogRecordProcessor(log_exporter))\n",
    "# Optionally, add a console exporter for local debugging\n",
    "logger_provider.add_log_record_processor(SimpleLogRecordProcessor(ConsoleLogExporter()))\n",
    "# Sets the global default logger provider\n",
    "set_logger_provider(logger_provider)\n",
    "\n",
    "# Create a logging handler to write logging records, in OTLP format, to the exporter.\n",
    "handler = LoggingHandler()\n",
    "# Add filters to the handler to only process records from semantic_kernel.\n",
    "handler.addFilter(logging.Filter(\"semantic_kernel\"))\n",
    "# Attach the handler to the root logger. `getLogger()` with no arguments returns the root logger.\n",
    "# Events from all child loggers will be processed by this handler.\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f824b9",
   "metadata": {},
   "source": [
    "## Setup metrics (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2db2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a metric provider for the application. This is a factory for creating meters.\n",
    "from opentelemetry.metrics import set_meter_provider\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk.metrics.view import DropAggregation, View\n",
    "from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\n",
    "# OTEL metrics exporter\n",
    "from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\n",
    "\n",
    "exporter = OTLPMetricExporter()\n",
    "\n",
    "meter_provider = MeterProvider(\n",
    "    metric_readers=[PeriodicExportingMetricReader(exporter, export_interval_millis=5000)],\n",
    "    views=[\n",
    "        # Dropping all instrument names except for those starting with \"semantic_kernel\"\n",
    "        View(instrument_name=\"*\", aggregation=DropAggregation()),\n",
    "        View(instrument_name=\"semantic_kernel*\"),\n",
    "    ],\n",
    ")\n",
    "# Sets the global default meter provider\n",
    "set_meter_provider(meter_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a43af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SK environment variables are set, otherwise raise an error since SK won't write to OTLP\n",
    "import os\n",
    "assert os.getenv(\"SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS_SENSITIVE\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AsyncAzureOpenAI\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(\n",
    "    credential, \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "\n",
    "client = AsyncAzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "service = AzureChatCompletion(\n",
    "        deployment_name=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "        async_client=client,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee143ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.agents.chat_completion.chat_completion_agent import ChatHistoryAgentThread\n",
    "\n",
    "thread = ChatHistoryAgentThread()\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "    id=\"chat-completion-agent\",\n",
    "    name=\"ChatCompletionAgent\",\n",
    "    service=service,\n",
    "    instructions=\"\"\"You are a helpful assistant. You tell jokes\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ebc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for r in agent.invoke(messages=\"A joke about developers\", thread=thread):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67407a8",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "## 1. Logs rendered as spans\n",
    "![Logs rendered as traces](./logs_as_traces.png)\n",
    "\n",
    "## 2. Default logs\n",
    "![Default logs](./default_logs.png)\n",
    "\n",
    "![Logs table](./logs_table.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
